{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercice : Prédiction de survie sur le Titanic\n",
    "\n",
    "Adaboost est généralement utilisé pour les problémes de classification. Dans notre cas, on va prédire la survie avec le dataset Titanic.\n",
    "\n",
    "Reprenez le dataframe utilisé dans l'implementation avec les arbres de décision pour cet exercice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pclass  survived     sex        age\n",
      "0        1.0       1.0  female  29.000000\n",
      "1        1.0       1.0    male   0.916700\n",
      "2        1.0       0.0  female   2.000000\n",
      "3        1.0       0.0    male  30.000000\n",
      "4        1.0       0.0  female  25.000000\n",
      "...      ...       ...     ...        ...\n",
      "1305     3.0       0.0  female  29.881135\n",
      "1306     3.0       0.0    male  26.500000\n",
      "1307     3.0       0.0    male  27.000000\n",
      "1308     3.0       0.0    male  29.000000\n",
      "1309     NaN       NaN     NaN  29.881135\n",
      "\n",
      "[1310 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Charger le dataset\n",
    "data = pd.read_csv('./datasets/titanic.csv') \n",
    "data= data.drop(['name', 'sibsp', 'parch', 'ticket', 'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'], axis=1)\n",
    "\n",
    "#remplace les valeurs manquantes de age par une moyenne\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "#impute à colonne 'age'\n",
    "data['age'] = imputer.fit_transform(data[['age']]).ravel()\n",
    "\n",
    "#verifier dataset \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      pclass  survived  sex        age\n",
      "1309     NaN       NaN  NaN  29.881135\n"
     ]
    }
   ],
   "source": [
    "#check la ligne où il y a Nan\n",
    "\n",
    "ligne_nan = data[data.isna().any(axis=1)]\n",
    "print(ligne_nan)\n",
    "\n",
    "# où est l'index de la ligne avec NaN\n",
    "index_nan = data[data.isna().all(axis=1)].index\n",
    "\n",
    "\n",
    "data = data.dropna()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X = data.drop(\"survived\", axis=1)\n",
    "y = data['survived']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=data['sex'])\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "data['sex'] = label_encoder.fit_transform(data['sex'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass: 0.08\n",
      "survived: 0.02\n",
      "sex: 0.9\n",
      "Accuracy: 0.7404580152671756\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(n_estimators=100, learning_rate=1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# importance des features\n",
    "feature_importance = model.feature_importances_\n",
    "\n",
    "# features et leur nom\n",
    "feature_importance_dict = dict(zip(data.columns, feature_importance))\n",
    "\n",
    "# affiche features et nom\n",
    "for feature, importance in feature_importance_dict.items():\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "accuracy_titanic = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:',accuracy_titanic) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour ceux qui ont terminé, \n",
    "\n",
    "data_apple = pd.read_csv('./datasets/apple_quality.csv') \n",
    "\n",
    "#verifier dataset \n",
    "print(data_apple)\n",
    "\n",
    "data_apple.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Régression logistique :\n",
      "Précision du modèle : 0.7862595419847328\n",
      "Matrice de confusion :\n",
      " [[140  29]\n",
      " [ 27  66]]\n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.83      0.83       169\n",
      "         1.0       0.69      0.71      0.70        93\n",
      "\n",
      "    accuracy                           0.79       262\n",
      "   macro avg       0.77      0.77      0.77       262\n",
      "weighted avg       0.79      0.79      0.79       262\n",
      "\n",
      "\n",
      "AdaBoost :\n",
      "Précision du modèle : 0.7480916030534351\n",
      "Matrice de confusion :\n",
      " [[127  42]\n",
      " [ 24  69]]\n",
      "Rapport de classification :\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.84      0.75      0.79       169\n",
      "         1.0       0.62      0.74      0.68        93\n",
      "\n",
      "    accuracy                           0.75       262\n",
      "   macro avg       0.73      0.75      0.74       262\n",
      "weighted avg       0.76      0.75      0.75       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# Traitement des données\n",
    "label_encoder = LabelEncoder()\n",
    "X = data.drop(\"survived\", axis=1)\n",
    "y = data['survived']\n",
    "\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=data['sex'])\n",
    "\n",
    "# Régression logistique\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "logistic_regression_predictions = logistic_regression_model.predict(X_test)\n",
    "\n",
    "# AdaBoost\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "adaboost_predictions = adaboost_model.predict(X_test)\n",
    "\n",
    "# Évaluer la performance des modèles\n",
    "accuracy_logistic_regression = accuracy_score(y_test, logistic_regression_predictions)\n",
    "accuracy_adaboost = accuracy_score(y_test, adaboost_predictions)\n",
    "\n",
    "conf_matrix_logistic_regression = confusion_matrix(y_test, logistic_regression_predictions)\n",
    "conf_matrix_adaboost = confusion_matrix(y_test, adaboost_predictions)\n",
    "\n",
    "classification_rep_logistic_regression = classification_report(y_test, logistic_regression_predictions)\n",
    "classification_rep_adaboost = classification_report(y_test, adaboost_predictions)\n",
    "\n",
    "# Afficher les résultats\n",
    "print(\"Régression logistique :\")\n",
    "print(f\"Précision du modèle : {accuracy_logistic_regression}\")\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix_logistic_regression)\n",
    "print(\"Rapport de classification :\\n\", classification_rep_logistic_regression)\n",
    "\n",
    "print(\"\\nAdaBoost :\")\n",
    "print(f\"Précision du modèle : {accuracy_adaboost}\")\n",
    "print(\"Matrice de confusion :\\n\", conf_matrix_adaboost)\n",
    "print(\"Rapport de classification :\\n\", classification_rep_adaboost)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (RandomForest): 0.7799043062200957\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'property' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 47>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy (RandomForest):\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_rf)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Accéder à l'impureté de Gini au nœud racine\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m root_gini \u001b[38;5;241m=\u001b[39m \u001b[43mPipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtree_\u001b[38;5;241m.\u001b[39mimpurity[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImpurity (Gini) at root node:\u001b[39m\u001b[38;5;124m\"\u001b[39m, root_gini)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Accéder à la profondeur de l'arbre de décision\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'property' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = 'datasets/titanic.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Sélection des colonnes pertinentes\n",
    "selected_columns = [\"pclass\", \"sex\", \"age\", \"sibsp\", \"parch\", \"fare\", \"embarked\", \"survived\"]\n",
    "df = df[selected_columns]\n",
    "\n",
    "# Suppression des lignes avec des valeurs manquantes\n",
    "df = df.dropna()\n",
    "\n",
    "# Séparation des features et de la variable cible\n",
    "X = df.drop(\"survived\", axis=1)\n",
    "y = df[\"survived\"]\n",
    "\n",
    "# Définition des colonnes catégoriques et numériques\n",
    "categorical_cols = [\"sex\", \"embarked\"]\n",
    "numeric_cols = [\"pclass\", \"age\", \"sibsp\", \"parch\", \"fare\"]\n",
    "\n",
    "# Création du preprocessor pour transformer les données\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", \"passthrough\", numeric_cols),\n",
    "        (\"cat\", OneHotEncoder(), categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Création du pipeline avec le preprocessor et le modèle RandomForestClassifier\n",
    "pipeline_rf = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", RandomForestClassifier(n_estimators=100, max_samples= 0.9, random_state=42))\n",
    "])\n",
    "\n",
    "# Division du jeu de données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42,)\n",
    "\n",
    "# Ajustement du modèle sur les données d'entraînement\n",
    "pipeline_rf.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions sur l'ensemble de test\n",
    "y_pred_rf = pipeline_rf.predict(X_test)\n",
    "\n",
    "# Calcul de l'accuracy pour le modèle RandomForest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "print(\"Accuracy (RandomForest):\", accuracy_rf)\n",
    "\n",
    "# Accéder à l'impureté de Gini au nœud racine\n",
    "root_gini = Pipeline.named_steps['model'].tree_.impurity[0]\n",
    "print(\"Impurity (Gini) at root node:\", root_gini)\n",
    "\n",
    "# Accéder à la profondeur de l'arbre de décision\n",
    "tree_depth = Pipeline.named_steps['model'].tree_.max_depth\n",
    "print(\"Profondeur de l'arbre de décision:\", tree_depth)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
