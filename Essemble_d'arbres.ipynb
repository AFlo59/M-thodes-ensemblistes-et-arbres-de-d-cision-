{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ensemble d'Arbres (Random Forest) :\n",
    "\n",
    "Fonctionnement Théorique :\n",
    "Introduction :\n",
    "Un Random Forest est un ensemble d'arbres de décision. Au lieu d'avoir un seul arbre, on en construit plusieurs, et la prédiction finale est obtenue en moyennant (pour la régression) ou en votant (pour la classification) les prédictions de chaque arbre.\n",
    "\n",
    "Étape 1 : Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "Exemple : Imagine que tu as un sac avec des boules numérotées de 1 à 10. Tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "\n",
    "Étape 2 : Construction des Arbres :\n",
    "\n",
    "Théorie : Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "Exemple : Quand tu poses des questions, tu ne considères pas toujours les mêmes caractéristiques pour chaque arbre. Par exemple, pour un arbre, tu peux demander \"a-t-il des ailes?\" et pour un autre \"a-t-il des pattes?\".\n",
    "\n",
    "Étape 3 : Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "\n",
    "Hyperparamètres et Stratégie :\n",
    "Nombre d'Arbres (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total d'arbres dans le Random Forest.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 arbres.\n",
    "Cas Spécifiques : Plus d'arbres améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des arbres.\n",
    "Min. d'Échantillons pour Diviser (min_samples_split) :\n",
    "\n",
    "Importance : Le nombre minimum d'échantillons nécessaires pour effectuer une division.\n",
    "Ordre de Grandeur : Généralement petit, comme 2 à 5.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la complexité des arbres.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "Régression : Oui, les Random Forest sont couramment utilisés pour la régression. La prédiction finale est la moyenne des prédictions des arbres.\n",
    "\n",
    "Classification : Oui, les Random Forest sont également très efficaces pour la classification. La prédiction finale est déterminée par le vote majoritaire des arbres.\n",
    "\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Exemple pour la Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "Ces exemples de code illustrent comment utiliser un Random Forest pour la régression et la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) :\n",
    "## Introduction :\n",
    "Le Bagging est une technique d'ensemble qui vise à améliorer la stabilité et la précision des modèles en combinant les prédictions de plusieurs modèles indépendants. Il utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "### Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "Exemple : Si tu as un sac avec des boules numérotées de 1 à 10, tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "Entraînement Indépendant :\n",
    "\n",
    "Théorie : Chaque modèle est entraîné indépendamment sur son ensemble de données d'entraînement spécifique.\n",
    "Exemple : Imaginons que chaque modèle représente un ami qui essaye de deviner quelque chose. Chacun d'eux a un ensemble d'informations légèrement différent.\n",
    "Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque modèle sont combinées par moyenne (pour la régression) ou par vote majoritaire (pour la classification).\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "### Nombre de Modèles (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total de modèles dans l'ensemble.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 modèles.\n",
    "Cas Spécifiques : Plus de modèles améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre dans les modèles.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division dans les arbres.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des modèles.\n",
    "# Pasting (Bagging sans Bootstrap Sampling) :\n",
    "## Introduction :\n",
    "Le Pasting est similaire au Bagging, mais au lieu d'utiliser Bootstrap Sampling, il utilise un échantillonnage sans remplacement pour créer les ensembles d'entraînement pour chaque modèle.\n",
    "\n",
    "## Différence avec le Bagging :\n",
    "\n",
    "Bagging : Utilise Bootstrap Sampling (tirage avec remplacement).\n",
    "Pasting : Utilise un échantillonnage sans remplacement.\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "Le fonctionnement du Pasting est essentiellement le même que le Bagging, mais au lieu de permettre aux mêmes échantillons de se retrouver dans plusieurs ensembles d'entraînement, chaque échantillon ne peut être dans qu'un seul ensemble.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "Les hyperparamètres et la stratégie pour le Pasting sont essentiellement les mêmes que pour le Bagging.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "\n",
    "Le Bagging et le Pasting peuvent être utilisés aussi bien pour la régression que pour la classification, en fonction du type de modèle utilisé pour chaque base learner (modèle de base) dans l'ensemble.\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression avec Bagging\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_regressor = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=100, random_state=42)\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "predictions = bagging_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error (Bagging): {mse}')\n",
    "\n",
    "# Exemple pour la Classification avec Pasting\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "pasting_classifier = BaggingClassifier(base_classifier, n_estimators=100, bootstrap=False, random_state=42)\n",
    "pasting_classifier.fit(X_train, y_train)\n",
    "predictions = pasting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy (Pasting): {accuracy}')\n",
    "Ces exemples de code montrent comment utiliser le Bagging pour la régression et le Pasting pour la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "les limite de l'accuracy : classe imbalance dummyclassifier de sklearn \n",
    "matrice de confusion : \n",
    "recall = tp / (tp +fn)\n",
    "precision = tp / (tp +fp)\n",
    "f1 score = 2*(presicion * recall)/(precision + recall) = 2tp/ (\n",
    "\n",
    ")\n",
    "spécificité = tn / fp+tn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
