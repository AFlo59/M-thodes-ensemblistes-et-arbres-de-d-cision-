{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy du modèle Bagging : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Charger l'ensemble de données Iris\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Créer un DataFrame pandas avec les données\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target  # Ajouter la colonne cible\n",
    "\n",
    "# Enregistrer le DataFrame en tant que fichier CSV\n",
    "iris_df.to_csv('iris_df.csv', index=False)\n",
    "# Charger les données\n",
    "iris_df = pd.read_csv('iris_df.csv')\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = iris_df.drop('target', axis=1)\n",
    "y = iris_df['target']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer un modèle d'arbre de décision\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Créer un modèle Bagging avec des arbres de décision\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=5, random_state=42)\n",
    "\n",
    "# Entraîner le modèle Bagging\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Évaluer la performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy du modèle Bagging : {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Théorique : Ensembles d'Arbres\n",
    "### Introduction :\n",
    "Les ensembles d'arbres sont une stratégie puissante en apprentissage automatique, où plusieurs modèles d'arbres sont combinés pour améliorer la performance globale. Deux techniques couramment utilisées dans les ensembles d'arbres sont le Bagging (Bootstrap Aggregating) et le Pasting.\n",
    "\n",
    "![Schéma Ensembles d'Arbres](assets/shema_bagging&plasting/Shema_Baggin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating) :\n",
    "Le Bagging est une technique qui vise à réduire la variance en agrégeant plusieurs modèles appris à partir de sous-ensembles de données générés par échantillonnage avec remplacement (bootstrap) du dataset d'origine. Voici les étapes générales du Bagging :\n",
    "\n",
    "#### Idée Principale :\n",
    "Utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "#### Fonctionnement :\n",
    "Chaque modèle est formé indépendamment sur son propre ensemble d'entraînement, créé en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Avantages :\n",
    "Réduit la variance, améliore la stabilité, et prévient le surajustement.\n",
    "\n",
    "### Étape 1 : Bootstrap Sampling :\n",
    "#### Théorie :\n",
    "On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Exemple :\n",
    "Supposons que nous ayons 150 exemples dans le dataset Iris. Nous créons plusieurs ensembles en tirant aléatoirement 150 exemples avec remplacement à chaque fois.\n",
    "\n",
    "### Étape 2 : Construction des Arbres :\n",
    "#### Théorie :\n",
    "Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "#### Exemple :\n",
    "Quand on construit un arbre sur un sous-ensemble d'Iris, on pourrait, par exemple, considérer uniquement la longueur du sépale et la largeur du pétale pour chaque nœud.\n",
    "\n",
    "### Étape 3 : Entraînement d'Arbres Indépendants :\n",
    "#### Théorie :\n",
    "Chaque arbre est entraîné de manière indépendante sur son propre ensemble d'entraînement généré par Bootstrap Sampling.\n",
    "\n",
    "#### Exemple :\n",
    "Pour chaque sous-ensemble d'Iris créé, un modèle d'arbre de décision est formé.\n",
    "\n",
    "### Étape 4 : Agrégation des Prédictions :\n",
    "#### Théorie :\n",
    "Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "#### Exemple :\n",
    "Si nous avons 5 modèles d'arbres, nous agrégeons leurs prédictions (par vote majoritaire, moyenne, etc.) pour obtenir la prédiction finale pour une observation Iris spécifique.\n",
    "\n",
    "## Conclusion :\n",
    "Le Bagging, en utilisant des modèles d'arbres de décision, offre une méthode robuste pour améliorer la performance des modèles et prévenir le surajustement, tout en utilisant le dataset Iris comme exemple spécifique. Chaque étape du processus contribue à la diversité des modèles, renforçant ainsi la capacité de généralisation de l'ensemble dans le contexte d'Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasting :\n",
    "Pasting est une technique similaire au Bagging, mais au lieu d'utiliser un échantillonnage avec remplacement (bootstrap), elle utilise un échantillonnage sans remplacement pour créer différents ensembles d'entraînement pour chaque modèle. Voici les étapes générales du Pasting :\n",
    "\n",
    "#### Idée Principale :\n",
    "Utilise l'échantillonnage sans remplacement pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "#### Fonctionnement :\n",
    "Chaque modèle est formé indépendamment sur son propre ensemble d'entraînement, créé en tirant aléatoirement sans remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Avantages :\n",
    "Réduit la variance, améliore la stabilité, et prévient le surajustement.\n",
    "\n",
    "### Étape 1 : Pasting (Échantillonnage sans remplacement) :\n",
    "#### Théorie :\n",
    "On crée plusieurs ensembles d'entraînement en tirant aléatoirement sans remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Supposons que nous ayons 150 exemples dans le dataset Iris. Nous créons plusieurs ensembles en tirant aléatoirement, sans remplacement, 150 exemples à chaque fois.\n",
    "\n",
    "### Étape 2 : Construction des Arbres :\n",
    "#### Théorie :\n",
    "Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "#### Exemple :\n",
    "Quand on construit un arbre sur un sous-ensemble d'Iris, on pourrait, par exemple, considérer uniquement la longueur du sépale et la largeur du pétale pour chaque nœud.\n",
    "\n",
    "### Étape 3 : Entraînement d'Arbres Indépendants :\n",
    "#### Théorie :\n",
    "Chaque arbre est entraîné de manière indépendante sur son propre ensemble d'entraînement généré par Pasting.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Pour chaque sous-ensemble d'Iris créé, un modèle d'arbre de décision est formé.\n",
    "\n",
    "### Étape 4 : Agrégation des Prédictions :\n",
    "#### Théorie :\n",
    "Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Si nous avons 5 modèles d'arbres, nous agrégeons leurs prédictions (par vote majoritaire, moyenne, etc.) pour obtenir la prédiction finale pour une observation Iris spécifique.\n",
    "\n",
    "## Conclusion :\n",
    "Le Pasting, tout comme le Bagging, est une technique qui utilise des modèles d'arbres de décision pour créer un ensemble robuste et généralisable. En utilisant l'échantillonnage sans remplacement, Pasting offre une alternative au Bagging et peut être particulièrement utile lorsque le dataset est limité et que l'on souhaite éviter la répétition d'observations dans les ensembles d'entraînement. Chaque étape du processus contribue à la diversité des modèles, renforçant ainsi la capacité de généralisation de l'ensemble dans le contexte d'Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parlons maintenant du \"Voting\" (ou vote) dans le monde des arbres et des amis.\n",
    "\n",
    " \n",
    "\n",
    "### Voting (Vote) :\n",
    "\n",
    " \n",
    "\n",
    "Imagine que tu veux choisir quelle glace manger, et tu as trois amis qui ont chacun leur propre suggestion.\n",
    "\n",
    " \n",
    "\n",
    "Ami 1 : Il suggère la glace à la vanille.\n",
    "\n",
    "Ami 2 : Il suggère la glace au chocolat.\n",
    "\n",
    "Ami 3 : Il suggère la glace à la fraise.\n",
    "\n",
    "Maintenant, le \"Voting\" (ou vote) consiste simplement à choisir la suggestion qui obtient le plus de votes.\n",
    "\n",
    " \n",
    "\n",
    "Si plus d'amis préfèrent la vanille, alors tu choisis la glace à la vanille.\n",
    "\n",
    "Si plus d'amis préfèrent le chocolat, alors tu choisis la glace au chocolat.\n",
    "\n",
    "Si plus d'amis préfèrent la fraise, alors tu choisis la glace à la fraise.\n",
    "\n",
    "Le \"Voting\" dans les modèles d'arbres fonctionne de manière similaire. Imaginons que tu as plusieurs modèles d'arbres, chacun donnant sa propre prédiction.\n",
    "\n",
    " \n",
    "\n",
    "Modèle 1 : Il prédit \"Oui\".\n",
    "\n",
    "Modèle 2 : Il prédit \"Non\".\n",
    "\n",
    "Modèle 3 : Il prédit \"Oui\".\n",
    "\n",
    "Le \"Voting\" peut être \"majoritaire\" ou \"à la moyenne\".\n",
    "\n",
    " \n",
    "\n",
    "#### Majoritaire : Si plus de modèles disent \"Oui\", alors la réponse finale est \"Oui\". Sinon, c'est \"Non\".\n",
    "\n",
    "#### Moyenne : Si tu regardes les réponses chiffrées (par exemple, des probabilités), tu peux prendre la moyenne de ces chiffres pour obtenir une réponse finale.\n",
    "\n",
    "C'est un peu comme écouter tes amis et suivre la suggestion qui obtient le plus de votes. Dans le monde des modèles d'arbres, le \"Voting\" te donne une décision finale en prenant en compte l'avis de plusieurs modèles. C'est une façon de rassembler les idées de différents arbres pour faire le meilleur choix possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamètres Importants :\n",
    "\n",
    "#### Nombre de Modèles (n_estimators) : \n",
    "Le nombre total d'arbres dans l'ensemble.\n",
    "\n",
    "#### Profondeur d'Arbre (max_depth) : \n",
    "La profondeur maximale de chaque arbre.\n",
    "\n",
    "#### Nombre de Caractéristiques à Considérer (max_features) : \n",
    "Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "\n",
    "#### Critère de Division (criterion) :\n",
    "Indique comment mesurer la qualité d'une division (ex. Gini pour la classification, MSE pour la régression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les hyperparamètres sont des paramètres externes au modèle qui influent sur le processus d'apprentissage mais qui ne sont pas appris par le modèle lui-même. Leur ajustement peut significativement affecter la performance du modèle. Voici quelques explications sur le fonctionnement des hyperparamètres, leur ordre de grandeur et comment les choisir :\n",
    "1. Hyperparamètres Communs dans les Arbres de Décision et les Ensembles (Bagging, Pasting, Random Forest) :\n",
    "\n",
    "    n_estimators : Il s'agit du nombre d'arbres dans l'ensemble. Il est important de trouver un équilibre, car augmenter le nombre d'arbres peut améliorer la performance jusqu'à un certain point, mais cela entraîne également un coût en termes de temps de calcul.\n",
    "\n",
    "    max_depth : La profondeur maximale de chaque arbre. Cela contrôle la complexité du modèle. Une valeur trop élevée peut conduire à un surajustement, tandis qu'une valeur trop faible peut entraîner un sous-ajustement.\n",
    "\n",
    "    min_samples_split : Le nombre minimum d'échantillons requis pour diviser un nœud. Une valeur plus élevée prévient le surajustement.\n",
    "\n",
    "    min_samples_leaf : Le nombre minimum d'échantillons requis pour être dans une feuille. Comme min_samples_split, il aide à contrôler le surajustement.\n",
    "\n",
    "    max_features : Le nombre maximal de fonctionnalités à considérer pour la division d'un nœud. Cela permet de rendre le modèle plus robuste en introduisant plus de diversité.\n",
    "\n",
    "2. Ordre de Grandeur des Hyperparamètres :\n",
    "\n",
    "    n_estimators : Typiquement dans la plage de 50 à quelques centaines.\n",
    "\n",
    "    max_depth : Dépend de la taille du jeu de données et de la complexité de la relation à modéliser. Peut aller de quelques unités à plusieurs dizaines.\n",
    "\n",
    "    min_samples_split et min_samples_leaf : Souvent dans la plage de 1 à 20.\n",
    "\n",
    "    max_features : Peut être réglé sur \"auto\" (équivalent à sqrt(n_features)), \"log2\", ou un nombre fixe.\n",
    "\n",
    "3. Choix des Hyperparamètres :\n",
    "\n",
    "    Recherche Manuelle : Commencez avec une valeur arbitraire, évaluez la performance du modèle, et ajustez les hyperparamètres en conséquence.\n",
    "\n",
    "    Recherche Grille (Grid Search) : Énumérez plusieurs combinaisons d'hyperparamètres et évaluez-les toutes. Sklearn propose la fonction GridSearchCV pour cela.\n",
    "\n",
    "    Recherche Aléatoire (Random Search) : Échantillonnez aléatoirement à partir d'une distribution définie pour chaque hyperparamètre. Peut être plus efficace que la recherche grille.\n",
    "\n",
    "    Optimisation Bayésienne : Utilise des méthodes d'optimisation bayésienne pour explorer de manière plus intelligente l'espace des hyperparamètres.\n",
    "\n",
    "4. Validation Croisée (Cross-Validation) :\n",
    "\n",
    "    Toujours utiliser la validation croisée pour évaluer la performance du modèle avec différentes combinaisons d'hyperparamètres.\n",
    "\n",
    "    Cela aide à éviter le surajustement aux données d'entraînement.\n",
    "\n",
    "5. Autres Conseils :\n",
    "\n",
    "    Visualisation : Utilisez des graphiques pour comprendre comment la performance du modèle change avec différents hyperparamètres.\n",
    "\n",
    "    Compréhension du Problème : La connaissance du domaine peut vous aider à choisir des valeurs initiales raisonnables pour les hyperparamètres.\n",
    "\n",
    "    Ressources en Calcul : Tenez compte du coût en termes de ressources de calcul lors du choix des valeurs pour n_estimators et d'autres hyperparamètres.\n",
    "\n",
    "En résumé, l'ajustement des hyperparamètres est un processus itératif et dépendant du problème. Il est essentiel de comprendre le comportement de chaque hyperparamètre et de les ajuster en fonction de la complexité du modèle et des caractéristiques du jeu de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ensemble d'Arbres (Random Forest) :\n",
    "\n",
    "Fonctionnement Théorique :\n",
    "Introduction :\n",
    "Un Random Forest est un ensemble d'arbres de décision. Au lieu d'avoir un seul arbre, on en construit plusieurs, et la prédiction finale est obtenue en moyennant (pour la régression) ou en votant (pour la classification) les prédictions de chaque arbre.\n",
    "\n",
    "Étape 1 : Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "Exemple : Imagine que tu as un sac avec des boules numérotées de 1 à 10. Tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "\n",
    "Étape 2 : Construction des Arbres :\n",
    "\n",
    "Théorie : Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "Exemple : Quand tu poses des questions, tu ne considères pas toujours les mêmes caractéristiques pour chaque arbre. Par exemple, pour un arbre, tu peux demander \"a-t-il des ailes?\" et pour un autre \"a-t-il des pattes?\".\n",
    "\n",
    "Étape 3 : Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "\n",
    "Hyperparamètres et Stratégie :\n",
    "Nombre d'Arbres (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total d'arbres dans le Random Forest.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 arbres.\n",
    "Cas Spécifiques : Plus d'arbres améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des arbres.\n",
    "Min. d'Échantillons pour Diviser (min_samples_split) :\n",
    "\n",
    "Importance : Le nombre minimum d'échantillons nécessaires pour effectuer une division.\n",
    "Ordre de Grandeur : Généralement petit, comme 2 à 5.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la complexité des arbres.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "Régression : Oui, les Random Forest sont couramment utilisés pour la régression. La prédiction finale est la moyenne des prédictions des arbres.\n",
    "\n",
    "Classification : Oui, les Random Forest sont également très efficaces pour la classification. La prédiction finale est déterminée par le vote majoritaire des arbres.\n",
    "\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Exemple pour la Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "Ces exemples de code illustrent comment utiliser un Random Forest pour la régression et la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) :\n",
    "## Introduction :\n",
    "Le Bagging est une technique d'ensemble qui vise à améliorer la stabilité et la précision des modèles en combinant les prédictions de plusieurs modèles indépendants. Il utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "### Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "Exemple : Si tu as un sac avec des boules numérotées de 1 à 10, tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "Entraînement Indépendant :\n",
    "\n",
    "Théorie : Chaque modèle est entraîné indépendamment sur son ensemble de données d'entraînement spécifique.\n",
    "Exemple : Imaginons que chaque modèle représente un ami qui essaye de deviner quelque chose. Chacun d'eux a un ensemble d'informations légèrement différent.\n",
    "Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque modèle sont combinées par moyenne (pour la régression) ou par vote majoritaire (pour la classification).\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "### Nombre de Modèles (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total de modèles dans l'ensemble.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 modèles.\n",
    "Cas Spécifiques : Plus de modèles améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre dans les modèles.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division dans les arbres.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des modèles.\n",
    "# Pasting (Bagging sans Bootstrap Sampling) :\n",
    "## Introduction :\n",
    "Le Pasting est similaire au Bagging, mais au lieu d'utiliser Bootstrap Sampling, il utilise un échantillonnage sans remplacement pour créer les ensembles d'entraînement pour chaque modèle.\n",
    "\n",
    "## Différence avec le Bagging :\n",
    "\n",
    "Bagging : Utilise Bootstrap Sampling (tirage avec remplacement).\n",
    "Pasting : Utilise un échantillonnage sans remplacement.\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "Le fonctionnement du Pasting est essentiellement le même que le Bagging, mais au lieu de permettre aux mêmes échantillons de se retrouver dans plusieurs ensembles d'entraînement, chaque échantillon ne peut être dans qu'un seul ensemble.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "Les hyperparamètres et la stratégie pour le Pasting sont essentiellement les mêmes que pour le Bagging.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "\n",
    "Le Bagging et le Pasting peuvent être utilisés aussi bien pour la régression que pour la classification, en fonction du type de modèle utilisé pour chaque base learner (modèle de base) dans l'ensemble.\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression avec Bagging\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_regressor = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=100, random_state=42)\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "predictions = bagging_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error (Bagging): {mse}')\n",
    "\n",
    "# Exemple pour la Classification avec Pasting\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "pasting_classifier = BaggingClassifier(base_classifier, n_estimators=100, bootstrap=False, random_state=42)\n",
    "pasting_classifier.fit(X_train, y_train)\n",
    "predictions = pasting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy (Pasting): {accuracy}')\n",
    "Ces exemples de code montrent comment utiliser le Bagging pour la régression et le Pasting pour la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wisdom of the crowd \n",
    "VotingClassifier & VotingRegressor\n",
    "BagginClassifier & BaginRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "les limite de l'accuracy : classe imbalance dummyclassifier de sklearn \n",
    "matrice de confusion : \n",
    "recall = tp / (tp +fn)\n",
    "precision = tp / (tp +fp)\n",
    "f1 score = 2*(presicion * recall)/(precision + recall) = 2tp/ (\n",
    "\n",
    ")\n",
    "spécificité = tn / fp+tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Bagging: 0.6\n",
      "Accuracy Pasting: 0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Création d'un dataset avec des caractéristiques variées\n",
    "data = {\n",
    "    'Taille': np.random.uniform(0.1, 2.0, 50),\n",
    "    'Poids': np.random.uniform(0.5, 100.0, 50),\n",
    "    'Type_Animal': np.random.choice(['Mammifère', 'Reptile', 'Oiseau', 'Amphibien'], size=50),\n",
    "    'Regime_Alimentaire': np.random.choice(['Carnivore', 'Omnivore', 'Herbivore'], size=50),\n",
    "    'Espece_Protegee': np.random.choice(['Oui', 'Non'], size=50),\n",
    "    'Comportement': np.random.choice(['Pacifique', 'Farouche', 'Territorial', 'Agressif', 'Prédateur'], size=50),\n",
    "    'Nom_Animal': [f'Animal_{i}' for i in range(50)],\n",
    "    'Dangerosite': np.random.randint(0, 11, size=50)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Définir la nouvelle variable cible en fonction des conditions spécifiées\n",
    "df['Dangerosite'] = np.where(\n",
    "    (df['Comportement'] == 'Pacifique'),  # Si le comportement est pacifique\n",
    "    0,\n",
    "    np.where(\n",
    "        ((df['Type_Animal'] == 'Oiseau') &\n",
    "        (df['Poids'] > 5) &\n",
    "        ((df['Comportement'] == 'Agressif') | (df['Comportement'] == 'Territorial'))),  # Si les conditions spécifiées pour les oiseaux sont satisfaites\n",
    "        10,\n",
    "        np.where(\n",
    "            ((df['Type_Animal'] == 'Reptile') | (df['Type_Animal'] == 'Amphibien')) &\n",
    "            ((df['Regime_Alimentaire'] == 'Carnivore') | ((df['Regime_Alimentaire'] == 'Omnivore') &\n",
    "            ((df['Comportement'] == 'Territorial') | (df['Comportement'] == 'Prédateur') | (df['Comportement'] == 'Agressif')))),  # Si les conditions spécifiées pour reptiles et amphibiens sont satisfaites\n",
    "            10,\n",
    "            np.where(\n",
    "                ((df['Type_Animal'] == 'Mammifère') | (df['Type_Animal'] == 'Reptile')) &\n",
    "                (df['Taille'] > 0.82) &\n",
    "                (df['Poids'] > 70) &\n",
    "                ((df['Regime_Alimentaire'] == 'Carnivore') |\n",
    "                 ((df['Regime_Alimentaire'] == 'Omnivore') & ((df['Comportement'] == 'Agressif') | (df['Comportement'] == 'Prédateur')))) &\n",
    "                (df['Espece_Protegee'] == 'Non'),  # Si les conditions spécifiées pour mammifères et reptiles sont satisfaites\n",
    "                10,\n",
    "                df['Dangerosite']  # Sinon, conserve la valeur existante\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Suppression des colonnes inutiles\n",
    "df = df.drop('Nom_Animal', axis=1)\n",
    "# Sauvegarder le DataFrame dans un fichier CSV\n",
    "df.to_csv('animal_dataset.csv', index=False)\n",
    "\n",
    "# Encodage one-hot des variables catégoriques\n",
    "df_encoded = pd.get_dummies(df, columns=['Type_Animal', 'Regime_Alimentaire', 'Espece_Protegee', 'Comportement'], drop_first=True)\n",
    "\n",
    "# Sauvegarder le DataFrame dans un fichier CSV\n",
    "df_encoded.to_csv('animal_encoded_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "# Divisons le dataset en ensembles d'entraînement et de test\n",
    "X_encoded = df_encoded.drop('Dangerosite', axis=1)\n",
    "y_encoded = df_encoded['Dangerosite']\n",
    "X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créons un classificateur de base (arbre de décision)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Entraînement avec Bagging\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=5, random_state=42)\n",
    "bagging_classifier.fit(X_train_encoded, y_train_encoded)\n",
    "bagging_accuracy = bagging_classifier.score(X_test_encoded, y_test_encoded)\n",
    "print(f\"Accuracy Bagging: {bagging_accuracy}\")\n",
    "\n",
    "# Entraînement avec Pasting\n",
    "pasting_classifier = BaggingClassifier(base_classifier, n_estimators=5, bootstrap=False, random_state=42)\n",
    "pasting_classifier.fit(X_train_encoded, y_train_encoded)\n",
    "pasting_accuracy = pasting_classifier.score(X_test_encoded, y_test_encoded)\n",
    "print(f\"Accuracy Pasting: {pasting_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
