
Ensemble d'Arbres (Random Forest) :

Fonctionnement Théorique :
Introduction :
Un Random Forest est un ensemble d'arbres de décision. Au lieu d'avoir un seul arbre, on en construit plusieurs, et la prédiction finale est obtenue en moyennant (pour la régression) ou en votant (pour la classification) les prédictions de chaque arbre.

Étape 1 : Bootstrap Sampling :

Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.

Exemple : Imagine que tu as un sac avec des boules numérotées de 1 à 10. Tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.

Étape 2 : Construction des Arbres :

Théorie : Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.

Exemple : Quand tu poses des questions, tu ne considères pas toujours les mêmes caractéristiques pour chaque arbre. Par exemple, pour un arbre, tu peux demander "a-t-il des ailes?" et pour un autre "a-t-il des pattes?".

Étape 3 : Agrégation :

Théorie : Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.

Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.

Hyperparamètres et Stratégie :
Nombre d'Arbres (n_estimators) :

Importance : Le nombre total d'arbres dans le Random Forest.
Ordre de Grandeur : Typiquement entre 50 et 500 arbres.
Cas Spécifiques : Plus d'arbres améliorent généralement la performance, mais cela nécessite plus de temps de calcul.
Profondeur d'Arbre (max_depth) :

Importance : La profondeur maximale de chaque arbre.
Ordre de Grandeur : Souvent entre 10 et 100.
Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.
Nombre de Caractéristiques à Considérer (max_features) :

Importance : Le nombre maximum de caractéristiques à considérer pour chaque division.
Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.
Cas Spécifiques : Ajuster cela peut contrôler la diversité des arbres.
Min. d'Échantillons pour Diviser (min_samples_split) :

Importance : Le nombre minimum d'échantillons nécessaires pour effectuer une division.
Ordre de Grandeur : Généralement petit, comme 2 à 5.
Cas Spécifiques : Ajuster cela peut contrôler la complexité des arbres.
Utilisation dans la Régression et la Classification :
Régression : Oui, les Random Forest sont couramment utilisés pour la régression. La prédiction finale est la moyenne des prédictions des arbres.

Classification : Oui, les Random Forest sont également très efficaces pour la classification. La prédiction finale est déterminée par le vote majoritaire des arbres.

Exemple de Code (en Python avec scikit-learn) :

python
Copy code
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, accuracy_score

# Exemple pour la Régression
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
regressor.fit(X_train, y_train)
predictions = regressor.predict(X_test)
mse = mean_squared_error(y_test, predictions)
print(f'Mean Squared Error: {mse}')

# Exemple pour la Classification
X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)
classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)
classifier.fit(X_train, y_train)
predictions = classifier.predict(X_test)
accuracy = accuracy_score(y_test, predictions)
print(f'Accuracy: {accuracy}')
Ces exemples de code illustrent comment utiliser un Random Forest pour la régression et la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème.
