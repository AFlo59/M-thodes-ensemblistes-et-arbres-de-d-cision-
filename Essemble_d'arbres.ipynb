{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy du modèle Bagging : 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "# Charger l'ensemble de données Iris\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Créer un DataFrame pandas avec les données\n",
    "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_df['target'] = iris.target  # Ajouter la colonne cible\n",
    "\n",
    "# Enregistrer le DataFrame en tant que fichier CSV\n",
    "iris_df.to_csv('iris_df.csv', index=False)\n",
    "# Charger les données\n",
    "iris_df = pd.read_csv('iris_df.csv')\n",
    "\n",
    "# Séparer les features et la cible\n",
    "X = iris_df.drop('target', axis=1)\n",
    "y = iris_df['target']\n",
    "\n",
    "# Diviser les données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créer un modèle d'arbre de décision\n",
    "base_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Créer un modèle Bagging avec des arbres de décision\n",
    "bagging_model = BaggingClassifier(base_model, n_estimators=5, random_state=42)\n",
    "\n",
    "# Entraîner le modèle Bagging\n",
    "bagging_model.fit(X_train, y_train)\n",
    "\n",
    "# Faire des prédictions\n",
    "y_pred = bagging_model.predict(X_test)\n",
    "\n",
    "# Évaluer la performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy du modèle Bagging : {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Théorique : Ensembles d'Arbres\n",
    "### Introduction :\n",
    "Les ensembles d'arbres sont une stratégie puissante en apprentissage automatique, où plusieurs modèles d'arbres sont combinés pour améliorer la performance globale. Deux techniques couramment utilisées dans les ensembles d'arbres sont le Bagging (Bootstrap Aggregating) et le Pasting.\n",
    "\n",
    "![Schéma Ensembles d'Arbres](assets/shema_bagging&plasting/Shema_Baggin.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregating) :\n",
    "Le Bagging est une technique qui vise à réduire la variance en agrégeant plusieurs modèles appris à partir de sous-ensembles de données générés par échantillonnage avec remplacement (bootstrap) du dataset d'origine. Voici les étapes générales du Bagging :\n",
    "\n",
    "#### Idée Principale :\n",
    "Utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "#### Fonctionnement :\n",
    "Chaque modèle est formé indépendamment sur son propre ensemble d'entraînement, créé en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Avantages :\n",
    "Réduit la variance, améliore la stabilité, et prévient le surajustement.\n",
    "\n",
    "### Étape 1 : Bootstrap Sampling :\n",
    "#### Théorie :\n",
    "On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Exemple :\n",
    "Supposons que nous ayons 150 exemples dans le dataset Iris. Nous créons plusieurs ensembles en tirant aléatoirement 150 exemples avec remplacement à chaque fois.\n",
    "\n",
    "### Étape 2 : Construction des Arbres :\n",
    "#### Théorie :\n",
    "Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "#### Exemple :\n",
    "Quand on construit un arbre sur un sous-ensemble d'Iris, on pourrait, par exemple, considérer uniquement la longueur du sépale et la largeur du pétale pour chaque nœud.\n",
    "\n",
    "### Étape 3 : Entraînement d'Arbres Indépendants :\n",
    "#### Théorie :\n",
    "Chaque arbre est entraîné de manière indépendante sur son propre ensemble d'entraînement généré par Bootstrap Sampling.\n",
    "\n",
    "#### Exemple :\n",
    "Pour chaque sous-ensemble d'Iris créé, un modèle d'arbre de décision est formé.\n",
    "\n",
    "### Étape 4 : Agrégation des Prédictions :\n",
    "#### Théorie :\n",
    "Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "#### Exemple :\n",
    "Si nous avons 5 modèles d'arbres, nous agrégeons leurs prédictions (par vote majoritaire, moyenne, etc.) pour obtenir la prédiction finale pour une observation Iris spécifique.\n",
    "\n",
    "## Conclusion :\n",
    "Le Bagging, en utilisant des modèles d'arbres de décision, offre une méthode robuste pour améliorer la performance des modèles et prévenir le surajustement, tout en utilisant le dataset Iris comme exemple spécifique. Chaque étape du processus contribue à la diversité des modèles, renforçant ainsi la capacité de généralisation de l'ensemble dans le contexte d'Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pasting :\n",
    "Pasting est une technique similaire au Bagging, mais au lieu d'utiliser un échantillonnage avec remplacement (bootstrap), elle utilise un échantillonnage sans remplacement pour créer différents ensembles d'entraînement pour chaque modèle. Voici les étapes générales du Pasting :\n",
    "\n",
    "#### Idée Principale :\n",
    "Utilise l'échantillonnage sans remplacement pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "#### Fonctionnement :\n",
    "Chaque modèle est formé indépendamment sur son propre ensemble d'entraînement, créé en tirant aléatoirement sans remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Avantages :\n",
    "Réduit la variance, améliore la stabilité, et prévient le surajustement.\n",
    "\n",
    "### Étape 1 : Pasting (Échantillonnage sans remplacement) :\n",
    "#### Théorie :\n",
    "On crée plusieurs ensembles d'entraînement en tirant aléatoirement sans remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Supposons que nous ayons 150 exemples dans le dataset Iris. Nous créons plusieurs ensembles en tirant aléatoirement, sans remplacement, 150 exemples à chaque fois.\n",
    "\n",
    "### Étape 2 : Construction des Arbres :\n",
    "#### Théorie :\n",
    "Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "#### Exemple :\n",
    "Quand on construit un arbre sur un sous-ensemble d'Iris, on pourrait, par exemple, considérer uniquement la longueur du sépale et la largeur du pétale pour chaque nœud.\n",
    "\n",
    "### Étape 3 : Entraînement d'Arbres Indépendants :\n",
    "#### Théorie :\n",
    "Chaque arbre est entraîné de manière indépendante sur son propre ensemble d'entraînement généré par Pasting.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Pour chaque sous-ensemble d'Iris créé, un modèle d'arbre de décision est formé.\n",
    "\n",
    "### Étape 4 : Agrégation des Prédictions :\n",
    "#### Théorie :\n",
    "Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "#### Exemple avec Iris :\n",
    "Si nous avons 5 modèles d'arbres, nous agrégeons leurs prédictions (par vote majoritaire, moyenne, etc.) pour obtenir la prédiction finale pour une observation Iris spécifique.\n",
    "\n",
    "## Conclusion :\n",
    "Le Pasting, tout comme le Bagging, est une technique qui utilise des modèles d'arbres de décision pour créer un ensemble robuste et généralisable. En utilisant l'échantillonnage sans remplacement, Pasting offre une alternative au Bagging et peut être particulièrement utile lorsque le dataset est limité et que l'on souhaite éviter la répétition d'observations dans les ensembles d'entraînement. Chaque étape du processus contribue à la diversité des modèles, renforçant ainsi la capacité de généralisation de l'ensemble dans le contexte d'Iris."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parlons maintenant du \"Voting\" (ou vote) dans le monde des arbres et des amis.\n",
    "\n",
    " \n",
    "\n",
    "### Voting (Vote) :\n",
    "\n",
    " \n",
    "\n",
    "Imagine que tu veux choisir quelle glace manger, et tu as trois amis qui ont chacun leur propre suggestion.\n",
    "\n",
    " \n",
    "\n",
    "Ami 1 : Il suggère la glace à la vanille.\n",
    "\n",
    "Ami 2 : Il suggère la glace au chocolat.\n",
    "\n",
    "Ami 3 : Il suggère la glace à la fraise.\n",
    "\n",
    "Maintenant, le \"Voting\" (ou vote) consiste simplement à choisir la suggestion qui obtient le plus de votes.\n",
    "\n",
    " \n",
    "\n",
    "Si plus d'amis préfèrent la vanille, alors tu choisis la glace à la vanille.\n",
    "\n",
    "Si plus d'amis préfèrent le chocolat, alors tu choisis la glace au chocolat.\n",
    "\n",
    "Si plus d'amis préfèrent la fraise, alors tu choisis la glace à la fraise.\n",
    "\n",
    "Le \"Voting\" dans les modèles d'arbres fonctionne de manière similaire. Imaginons que tu as plusieurs modèles d'arbres, chacun donnant sa propre prédiction.\n",
    "\n",
    " \n",
    "\n",
    "Modèle 1 : Il prédit \"Oui\".\n",
    "\n",
    "Modèle 2 : Il prédit \"Non\".\n",
    "\n",
    "Modèle 3 : Il prédit \"Oui\".\n",
    "\n",
    "Le \"Voting\" peut être \"majoritaire\" ou \"à la moyenne\".\n",
    "\n",
    " \n",
    "\n",
    "#### Majoritaire : Si plus de modèles disent \"Oui\", alors la réponse finale est \"Oui\". Sinon, c'est \"Non\".\n",
    "\n",
    "#### Moyenne : Si tu regardes les réponses chiffrées (par exemple, des probabilités), tu peux prendre la moyenne de ces chiffres pour obtenir une réponse finale.\n",
    "\n",
    "C'est un peu comme écouter tes amis et suivre la suggestion qui obtient le plus de votes. Dans le monde des modèles d'arbres, le \"Voting\" te donne une décision finale en prenant en compte l'avis de plusieurs modèles. C'est une façon de rassembler les idées de différents arbres pour faire le meilleur choix possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamètres Importants :\n",
    "\n",
    "#### Nombre de Modèles (n_estimators) : \n",
    "Le nombre total d'arbres dans l'ensemble.\n",
    "\n",
    "#### Profondeur d'Arbre (max_depth) : \n",
    "La profondeur maximale de chaque arbre.\n",
    "\n",
    "#### Nombre de Caractéristiques à Considérer (max_features) : \n",
    "Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "\n",
    "#### Critère de Division (criterion) :\n",
    "Indique comment mesurer la qualité d'une division (ex. Gini pour la classification, MSE pour la régression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les hyperparamètres sont des paramètres externes au modèle qui influent sur le processus d'apprentissage mais qui ne sont pas appris par le modèle lui-même. Leur ajustement peut significativement affecter la performance du modèle. Voici quelques explications sur le fonctionnement des hyperparamètres, leur ordre de grandeur et comment les choisir :\n",
    "1. Hyperparamètres Communs dans les Arbres de Décision et les Ensembles (Bagging, Pasting, Random Forest) :\n",
    "\n",
    "    n_estimators : Il s'agit du nombre d'arbres dans l'ensemble. Il est important de trouver un équilibre, car augmenter le nombre d'arbres peut améliorer la performance jusqu'à un certain point, mais cela entraîne également un coût en termes de temps de calcul.\n",
    "\n",
    "    max_depth : La profondeur maximale de chaque arbre. Cela contrôle la complexité du modèle. Une valeur trop élevée peut conduire à un surajustement, tandis qu'une valeur trop faible peut entraîner un sous-ajustement.\n",
    "\n",
    "    min_samples_split : Le nombre minimum d'échantillons requis pour diviser un nœud. Une valeur plus élevée prévient le surajustement.\n",
    "\n",
    "    min_samples_leaf : Le nombre minimum d'échantillons requis pour être dans une feuille. Comme min_samples_split, il aide à contrôler le surajustement.\n",
    "\n",
    "    max_features : Le nombre maximal de fonctionnalités à considérer pour la division d'un nœud. Cela permet de rendre le modèle plus robuste en introduisant plus de diversité.\n",
    "\n",
    "2. Ordre de Grandeur des Hyperparamètres :\n",
    "\n",
    "    n_estimators : Typiquement dans la plage de 50 à quelques centaines.\n",
    "\n",
    "    max_depth : Dépend de la taille du jeu de données et de la complexité de la relation à modéliser. Peut aller de quelques unités à plusieurs dizaines.\n",
    "\n",
    "    min_samples_split et min_samples_leaf : Souvent dans la plage de 1 à 20.\n",
    "\n",
    "    max_features : Peut être réglé sur \"auto\" (équivalent à sqrt(n_features)), \"log2\", ou un nombre fixe.\n",
    "\n",
    "3. Choix des Hyperparamètres :\n",
    "\n",
    "    Recherche Manuelle : Commencez avec une valeur arbitraire, évaluez la performance du modèle, et ajustez les hyperparamètres en conséquence.\n",
    "\n",
    "    Recherche Grille (Grid Search) : Énumérez plusieurs combinaisons d'hyperparamètres et évaluez-les toutes. Sklearn propose la fonction GridSearchCV pour cela.\n",
    "\n",
    "    Recherche Aléatoire (Random Search) : Échantillonnez aléatoirement à partir d'une distribution définie pour chaque hyperparamètre. Peut être plus efficace que la recherche grille.\n",
    "\n",
    "    Optimisation Bayésienne : Utilise des méthodes d'optimisation bayésienne pour explorer de manière plus intelligente l'espace des hyperparamètres.\n",
    "\n",
    "4. Validation Croisée (Cross-Validation) :\n",
    "\n",
    "    Toujours utiliser la validation croisée pour évaluer la performance du modèle avec différentes combinaisons d'hyperparamètres.\n",
    "\n",
    "    Cela aide à éviter le surajustement aux données d'entraînement.\n",
    "\n",
    "5. Autres Conseils :\n",
    "\n",
    "    Visualisation : Utilisez des graphiques pour comprendre comment la performance du modèle change avec différents hyperparamètres.\n",
    "\n",
    "    Compréhension du Problème : La connaissance du domaine peut vous aider à choisir des valeurs initiales raisonnables pour les hyperparamètres.\n",
    "\n",
    "    Ressources en Calcul : Tenez compte du coût en termes de ressources de calcul lors du choix des valeurs pour n_estimators et d'autres hyperparamètres.\n",
    "\n",
    "En résumé, l'ajustement des hyperparamètres est un processus itératif et dépendant du problème. Il est essentiel de comprendre le comportement de chaque hyperparamètre et de les ajuster en fonction de la complexité du modèle et des caractéristiques du jeu de données.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wisdom of the crowd \n",
    "VotingClassifier & VotingRegressor\n",
    "BagginClassifier & BaginRegressor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "les limite de l'accuracy : classe imbalance dummyclassifier de sklearn \n",
    "matrice de confusion : \n",
    "recall = tp / (tp +fn)\n",
    "precision = tp / (tp +fp)\n",
    "f1 score = 2*(presicion * recall)/(precision + recall) = 2tp/ (\n",
    "\n",
    ")\n",
    "spécificité = tn / fp+tn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
