{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Bagging: 0.6\n",
      "Accuracy Pasting: 0.5\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "# Création d'un dataset avec des caractéristiques variées\n",
    "data = {\n",
    "    'Taille': np.random.uniform(0.1, 2.0, 50),\n",
    "    'Poids': np.random.uniform(0.5, 100.0, 50),\n",
    "    'Type_Animal': np.random.choice(['Mammifère', 'Reptile', 'Oiseau', 'Amphibien'], size=50),\n",
    "    'Regime_Alimentaire': np.random.choice(['Carnivore', 'Omnivore', 'Herbivore'], size=50),\n",
    "    'Espece_Protegee': np.random.choice(['Oui', 'Non'], size=50),\n",
    "    'Comportement': np.random.choice(['Pacifique', 'Farouche', 'Territorial', 'Agressif', 'Prédateur'], size=50),\n",
    "    'Nom_Animal': [f'Animal_{i}' for i in range(50)],\n",
    "    'Dangerosite': np.random.randint(0, 11, size=50)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Définir la nouvelle variable cible en fonction des conditions spécifiées\n",
    "df['Dangerosite'] = np.where(\n",
    "    (df['Comportement'] == 'Pacifique'),  # Si le comportement est pacifique\n",
    "    0,\n",
    "    np.where(\n",
    "        ((df['Type_Animal'] == 'Oiseau') &\n",
    "        (df['Poids'] > 5) &\n",
    "        ((df['Comportement'] == 'Agressif') | (df['Comportement'] == 'Territorial'))),  # Si les conditions spécifiées pour les oiseaux sont satisfaites\n",
    "        10,\n",
    "        np.where(\n",
    "            ((df['Type_Animal'] == 'Reptile') | (df['Type_Animal'] == 'Amphibien')) &\n",
    "            ((df['Regime_Alimentaire'] == 'Carnivore') | ((df['Regime_Alimentaire'] == 'Omnivore') &\n",
    "            ((df['Comportement'] == 'Territorial') | (df['Comportement'] == 'Prédateur') | (df['Comportement'] == 'Agressif')))),  # Si les conditions spécifiées pour reptiles et amphibiens sont satisfaites\n",
    "            10,\n",
    "            np.where(\n",
    "                ((df['Type_Animal'] == 'Mammifère') | (df['Type_Animal'] == 'Reptile')) &\n",
    "                (df['Taille'] > 0.82) &\n",
    "                (df['Poids'] > 70) &\n",
    "                ((df['Regime_Alimentaire'] == 'Carnivore') |\n",
    "                 ((df['Regime_Alimentaire'] == 'Omnivore') & ((df['Comportement'] == 'Agressif') | (df['Comportement'] == 'Prédateur')))) &\n",
    "                (df['Espece_Protegee'] == 'Non'),  # Si les conditions spécifiées pour mammifères et reptiles sont satisfaites\n",
    "                10,\n",
    "                df['Dangerosite']  # Sinon, conserve la valeur existante\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Suppression des colonnes inutiles\n",
    "df = df.drop('Nom_Animal', axis=1)\n",
    "# Sauvegarder le DataFrame dans un fichier CSV\n",
    "df.to_csv('animal_dataset.csv', index=False)\n",
    "\n",
    "# Encodage one-hot des variables catégoriques\n",
    "df_encoded = pd.get_dummies(df, columns=['Type_Animal', 'Regime_Alimentaire', 'Espece_Protegee', 'Comportement'], drop_first=True)\n",
    "\n",
    "# Sauvegarder le DataFrame dans un fichier CSV\n",
    "df_encoded.to_csv('animal_encoded_dataset.csv', index=False)\n",
    "\n",
    "\n",
    "# Divisons le dataset en ensembles d'entraînement et de test\n",
    "X_encoded = df_encoded.drop('Dangerosite', axis=1)\n",
    "y_encoded = df_encoded['Dangerosite']\n",
    "X_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_encoded, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Créons un classificateur de base (arbre de décision)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "\n",
    "# Entraînement avec Bagging\n",
    "bagging_classifier = BaggingClassifier(base_classifier, n_estimators=5, random_state=42)\n",
    "bagging_classifier.fit(X_train_encoded, y_train_encoded)\n",
    "bagging_accuracy = bagging_classifier.score(X_test_encoded, y_test_encoded)\n",
    "print(f\"Accuracy Bagging: {bagging_accuracy}\")\n",
    "\n",
    "# Entraînement avec Pasting\n",
    "pasting_classifier = BaggingClassifier(base_classifier, n_estimators=5, bootstrap=False, random_state=42)\n",
    "pasting_classifier.fit(X_train_encoded, y_train_encoded)\n",
    "pasting_accuracy = pasting_classifier.score(X_test_encoded, y_test_encoded)\n",
    "print(f\"Accuracy Pasting: {pasting_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partie Théorique : Ensembles d'Arbres\n",
    "### Introduction :\n",
    "Les ensembles d'arbres sont une stratégie puissante en apprentissage automatique, où plusieurs modèles d'arbres sont combinés pour améliorer la performance globale. Deux techniques couramment utilisées dans les ensembles d'arbres sont le Bagging (Bootstrap Aggregating) et le Pasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Bagging (Bootstrap Aggregating) :\n",
    "![Schéma Bagging](assets\\shema_bagging&plasting\\Shema_Baggin.jpg)\n",
    "\n",
    "#### Idée Principale : \n",
    "Utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "#### Fonctionnement : \n",
    "Chaque modèle est formé indépendamment sur son propre ensemble d'entraînement, créé en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "#### Avantages :\n",
    " Réduit la variance, améliore la stabilité, et prévient le surajustement.\n",
    "\n",
    "\n",
    "##### Étape 1 : Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "Exemple : Imagine que tu as un sac avec des boules numérotées de 1 à 10. Tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "\n",
    "##### Étape 2 : Construction des Arbres :\n",
    "\n",
    "Théorie : Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "Exemple : Quand tu poses des questions, tu ne considères pas toujours les mêmes caractéristiques pour chaque arbre. Par exemple, pour un arbre, tu peux demander \"a-t-il des ailes?\" et pour un autre \"a-t-il des pattes?\".\n",
    "\n",
    "##### Étape 3 : Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pasting (Bagging sans Bootstrap Sampling) :\n",
    "\n",
    "#### Idée Principale : \n",
    "Similaire au Bagging, mais utilise un échantillonnage sans remplacement pour créer les ensembles d'entraînement.\n",
    "\n",
    "#### Différence avec le Bagging :\n",
    " Chaque échantillon ne peut être dans qu'un seul ensemble d'entraînement.\n",
    "\n",
    "#### Avantages : \n",
    "Réduit la variance, améliore la stabilité, et est utile lorsque le dataset est limité."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparamètres Importants :\n",
    "\n",
    "#### Nombre de Modèles (n_estimators) : \n",
    "Le nombre total d'arbres dans l'ensemble.\n",
    "\n",
    "#### Profondeur d'Arbre (max_depth) : \n",
    "La profondeur maximale de chaque arbre.\n",
    "\n",
    "#### Nombre de Caractéristiques à Considérer (max_features) : \n",
    "Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "\n",
    "#### Critère de Division (criterion) :\n",
    "Indique comment mesurer la qualité d'une division (ex. Gini pour la classification, MSE pour la régression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Ensemble d'Arbres (Random Forest) :\n",
    "\n",
    "Fonctionnement Théorique :\n",
    "Introduction :\n",
    "Un Random Forest est un ensemble d'arbres de décision. Au lieu d'avoir un seul arbre, on en construit plusieurs, et la prédiction finale est obtenue en moyennant (pour la régression) ou en votant (pour la classification) les prédictions de chaque arbre.\n",
    "\n",
    "Étape 1 : Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "\n",
    "Exemple : Imagine que tu as un sac avec des boules numérotées de 1 à 10. Tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "\n",
    "Étape 2 : Construction des Arbres :\n",
    "\n",
    "Théorie : Pour chaque échantillon, on construit un arbre de décision, mais à chaque nœud, on ne considère qu'un sous-ensemble aléatoire des caractéristiques.\n",
    "\n",
    "Exemple : Quand tu poses des questions, tu ne considères pas toujours les mêmes caractéristiques pour chaque arbre. Par exemple, pour un arbre, tu peux demander \"a-t-il des ailes?\" et pour un autre \"a-t-il des pattes?\".\n",
    "\n",
    "Étape 3 : Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque arbre sont combinées par moyenne (régression) ou vote majoritaire (classification) pour obtenir la prédiction finale.\n",
    "\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "\n",
    "Hyperparamètres et Stratégie :\n",
    "Nombre d'Arbres (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total d'arbres dans le Random Forest.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 arbres.\n",
    "Cas Spécifiques : Plus d'arbres améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des arbres.\n",
    "Min. d'Échantillons pour Diviser (min_samples_split) :\n",
    "\n",
    "Importance : Le nombre minimum d'échantillons nécessaires pour effectuer une division.\n",
    "Ordre de Grandeur : Généralement petit, comme 2 à 5.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la complexité des arbres.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "Régression : Oui, les Random Forest sont couramment utilisés pour la régression. La prédiction finale est la moyenne des prédictions des arbres.\n",
    "\n",
    "Classification : Oui, les Random Forest sont également très efficaces pour la classification. La prédiction finale est déterminée par le vote majoritaire des arbres.\n",
    "\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "regressor = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "predictions = regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Exemple pour la Classification\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "classifier.fit(X_train, y_train)\n",
    "predictions = classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "Ces exemples de code illustrent comment utiliser un Random Forest pour la régression et la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging (Bootstrap Aggregating) :\n",
    "## Introduction :\n",
    "Le Bagging est une technique d'ensemble qui vise à améliorer la stabilité et la précision des modèles en combinant les prédictions de plusieurs modèles indépendants. Il utilise le Bootstrap Sampling pour créer des ensembles d'entraînement différents pour chaque modèle.\n",
    "\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "### Bootstrap Sampling :\n",
    "\n",
    "Théorie : On crée plusieurs échantillons d'entraînement en tirant aléatoirement avec remplacement à partir du jeu de données original.\n",
    "Exemple : Si tu as un sac avec des boules numérotées de 1 à 10, tu pioches plusieurs fois, remets chaque boule tirée dans le sac, et répètes.\n",
    "Entraînement Indépendant :\n",
    "\n",
    "Théorie : Chaque modèle est entraîné indépendamment sur son ensemble de données d'entraînement spécifique.\n",
    "Exemple : Imaginons que chaque modèle représente un ami qui essaye de deviner quelque chose. Chacun d'eux a un ensemble d'informations légèrement différent.\n",
    "Agrégation :\n",
    "\n",
    "Théorie : Les prédictions de chaque modèle sont combinées par moyenne (pour la régression) ou par vote majoritaire (pour la classification).\n",
    "Exemple : Si tu demandes à plusieurs amis de deviner le nombre de bonbons dans un bocal, vous combinez ensuite vos réponses pour obtenir une estimation moyenne.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "### Nombre de Modèles (n_estimators) :\n",
    "\n",
    "Importance : Le nombre total de modèles dans l'ensemble.\n",
    "Ordre de Grandeur : Typiquement entre 50 et 500 modèles.\n",
    "Cas Spécifiques : Plus de modèles améliorent généralement la performance, mais cela nécessite plus de temps de calcul.\n",
    "Profondeur d'Arbre (max_depth) :\n",
    "\n",
    "Importance : La profondeur maximale de chaque arbre dans les modèles.\n",
    "Ordre de Grandeur : Souvent entre 10 et 100.\n",
    "Cas Spécifiques : Limiter la profondeur peut prévenir le surajustement.\n",
    "Nombre de Caractéristiques à Considérer (max_features) :\n",
    "\n",
    "Importance : Le nombre maximum de caractéristiques à considérer pour chaque division dans les arbres.\n",
    "Ordre de Grandeur : Racine carrée du nombre total de caractéristiques.\n",
    "Cas Spécifiques : Ajuster cela peut contrôler la diversité des modèles.\n",
    "# Pasting (Bagging sans Bootstrap Sampling) :\n",
    "## Introduction :\n",
    "Le Pasting est similaire au Bagging, mais au lieu d'utiliser Bootstrap Sampling, il utilise un échantillonnage sans remplacement pour créer les ensembles d'entraînement pour chaque modèle.\n",
    "\n",
    "## Différence avec le Bagging :\n",
    "\n",
    "Bagging : Utilise Bootstrap Sampling (tirage avec remplacement).\n",
    "Pasting : Utilise un échantillonnage sans remplacement.\n",
    "### Fonctionnement Théorique :\n",
    "\n",
    "Le fonctionnement du Pasting est essentiellement le même que le Bagging, mais au lieu de permettre aux mêmes échantillons de se retrouver dans plusieurs ensembles d'entraînement, chaque échantillon ne peut être dans qu'un seul ensemble.\n",
    "Hyperparamètres et Stratégie :\n",
    "\n",
    "Les hyperparamètres et la stratégie pour le Pasting sont essentiellement les mêmes que pour le Bagging.\n",
    "Utilisation dans la Régression et la Classification :\n",
    "\n",
    "Le Bagging et le Pasting peuvent être utilisés aussi bien pour la régression que pour la classification, en fonction du type de modèle utilisé pour chaque base learner (modèle de base) dans l'ensemble.\n",
    "Exemple de Code (en Python avec scikit-learn) :\n",
    "\n",
    "python\n",
    "Copy code\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Exemple pour la Régression avec Bagging\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_regressor = DecisionTreeRegressor(max_depth=10, random_state=42)\n",
    "bagging_regressor = BaggingRegressor(base_regressor, n_estimators=100, random_state=42)\n",
    "bagging_regressor.fit(X_train, y_train)\n",
    "predictions = bagging_regressor.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f'Mean Squared Error (Bagging): {mse}')\n",
    "\n",
    "# Exemple pour la Classification avec Pasting\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "base_classifier = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "pasting_classifier = BaggingClassifier(base_classifier, n_estimators=100, bootstrap=False, random_state=42)\n",
    "pasting_classifier.fit(X_train, y_train)\n",
    "predictions = pasting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(f'Accuracy (Pasting): {accuracy}')\n",
    "Ces exemples de code montrent comment utiliser le Bagging pour la régression et le Pasting pour la classification avec scikit-learn en Python. Vous pouvez ajuster les hyperparamètres selon les besoins spécifiques de votre problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X, y = make_blobs(n_samples=10000, n_features=10, centers=100,\n",
    "    random_state=0)\n",
    "\n",
    "clf = DecisionTreeClassifier(max_depth=None, min_samples_split=2,\n",
    "    random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean()\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=10, max_depth=None,\n",
    "    min_samples_split=2, random_state=0)\n",
    "scores = cross_val_score(clf, X, y, cv=5)\n",
    "scores.mean() > 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wisdom of the crowd \n",
    "VotingClassifier & VotingRegressor\n",
    "BagginClassifier & BaginRegressor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Mammifère'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m BaggingClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:338\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    330\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    331\u001b[0m     X,\n\u001b[0;32m    332\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    337\u001b[0m )\n\u001b[1;32m--> 338\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:473\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    470\u001b[0m seeds \u001b[38;5;241m=\u001b[39m random_state\u001b[38;5;241m.\u001b[39mrandint(MAX_INT, size\u001b[38;5;241m=\u001b[39mn_more_estimators)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seeds \u001b[38;5;241m=\u001b[39m seeds\n\u001b[1;32m--> 473\u001b[0m all_results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parallel_args\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_estimators\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseeds\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstarts\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_n_estimators\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# Reduce\u001b[39;00m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_ \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m    492\u001b[0m     itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(t[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m all_results)\n\u001b[0;32m    493\u001b[0m )\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[0;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_bagging.py:141\u001b[0m, in \u001b[0;36m_parallel_build_estimators\u001b[1;34m(n_estimators, ensemble, X, y, sample_weight, seeds, total_n_estimators, verbose, check_input)\u001b[0m\n\u001b[0;32m    138\u001b[0m         curr_sample_weight[not_indices_mask] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    140\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mestimator_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     X_ \u001b[38;5;241m=\u001b[39m X[indices][:, features] \u001b[38;5;28;01mif\u001b[39;00m requires_feature_indexing \u001b[38;5;28;01melse\u001b[39;00m X[indices]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:959\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    929\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    930\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    931\u001b[0m \n\u001b[0;32m    932\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m    957\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 959\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:242\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    238\u001b[0m check_X_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    239\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    240\u001b[0m )\n\u001b[0;32m    241\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 242\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_separately\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_y_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_missing_values_in_feature_mask(X)\n\u001b[0;32m    248\u001b[0m )\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\base.py:617\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    615\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_X_params:\n\u001b[0;32m    616\u001b[0m     check_X_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_X_params}\n\u001b[1;32m--> 617\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_X_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m check_y_params:\n\u001b[0;32m    619\u001b[0m     check_y_params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdefault_check_params, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params}\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:915\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    913\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 915\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    919\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Mammifère'"
     ]
    }
   ],
   "source": [
    "model = BaggingClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy\n",
    "\n",
    "les limite de l'accuracy : classe imbalance dummyclassifier de sklearn \n",
    "matrice de confusion : \n",
    "recall = tp / (tp +fn)\n",
    "precision = tp / (tp +fp)\n",
    "f1 score = 2*(presicion * recall)/(precision + recall) = 2tp/ (\n",
    "\n",
    ")\n",
    "spécificité = tn / fp+tn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
