{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qu'est ce que le boosting ?**\n",
    "\n",
    "C'est une méthode d\"ensemble qui vise à créer un modèle fort à partir de plusieurs modèles plus faible. Concrètement, cela combine les prédictions de plusieurs modèles. Ces modèles sont de préférences simples et faibles. Dans le but d'améliorer la robustesse et la précision du modèle final.\n",
    "\n",
    "**Comment cela fonctionne ?**\n",
    "\n",
    "Contrairement au bagging, le boosting va se construire de manière itérative, séquentielle. En fait, chaque modèle va essayer de corriger les erreurs de modèle n-1.\n",
    "\n",
    "A chaque itération, le boosting permet d'augmenter le poids des observations mal classées pour que le modèle suivant se concentre sur les cas difficiles. On peut voir cela comme une analyse qui augmente sa finesse au fur et à mesure, comme dans un entonnoir.\n",
    "\n",
    "Pour cela, on va utiliser des modèles faibles (\"Stumps\"). Concrètement, l'idée est de corriger progressivement au lieu d'utiliser un modèle de machine de guerre dès le départ.\n",
    "\n",
    "**Il existe deux types de boosting**\n",
    "\n",
    "- *AdaBoost(Adaptive Boosting)* \n",
    "- *Gradient Boosting*\n",
    "\n",
    "Dans ce notebook nous allons nous intéresser au Gradient Boosting. Il permet d'améliorer les prédictions en utilisant la descente de gradient pour minimiser les erreurs.\n",
    "Dans une autre partie du cours, nous verrons que nous pouvons utiliser *XGBoost, LightGBM et CatBoost* qui sont des variations avancées du gradient boosting.\n",
    "\n",
    "**Quels sont les avantages du boosting ?**\n",
    "\n",
    "- *Une précision plus grande et une bonne performance , autant sur des problèmes de classification que de régression.*\n",
    "\n",
    "- *Il corrige les erreurs et réduit la variance , cela réduit le biais gràce à la combinaison de plusieurs modèles*\n",
    "\n",
    "**Quels sont les limites ?**\n",
    "\n",
    "- *Le boosting peut provoqué un over-fitting s'il n'est pas bien régulé.*\n",
    "- *Il est sensible aux outliers*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Le Gradient Boosting*** est une technique avancée qui va construire un modèle prédictif robuste via plusieurs modèles plus faibles. On va faire cela avec des **arbres de décisions**. C'est comme une extension du Boosting.\n",
    "\n",
    "Donc on garde l'idée de base du Boosting et nous construirons des modèles de manières séquentielle (itératives) pour permettre la correction des erreurs par la somme des modèles précédents.\n",
    "\n",
    "***C'est ici la principale différence***\n",
    "Elle réside dans la façon dont les erreurs sont gérées. Comme son nom le suppose, on va utiliser la *descente de gradient* pour minimiser la fonction coût.\n",
    "\n",
    "*Concrètement comment ça marche ?*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'utilisation du Gradient Boosting sur le dataSet du Titanic est choix à plusieurs raisons. Tout d'abord la nature de l'ensemble de données et aux caractéristiques du modèle lui-même. En effet nous allons devoir gérer des features hétérogènes ( numériques et catégorielle). Le Gradient Boosting est efficace pour gérer ce type de données."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
